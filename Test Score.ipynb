{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Test Score.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"fUQNBHO5UL-r","colab_type":"text"},"cell_type":"markdown","source":["# Scoring your trained model\n","\n","In the cell below, please load your model into `model`. Also if you used an image size for your input images that *isn't* 224x224, you'll need to set `image_size` to the size you used. The scoring code assumes square input images.\n","\n","For example, this is how I loaded in my checkpoint:\n","\n","```python\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torchvision import models\n","\n","class FFClassifier(nn.Module):\n","    \n","    def __init__(self, in_features, hidden_features, \n","                       out_features, drop_prob=0.1):\n","        super().__init__()\n","        \n","        self.fc1 = nn.Linear(in_features, hidden_features)\n","        self.fc2 = nn.Linear(hidden_features, out_features)\n","        self.drop = nn.Dropout(p=drop_prob)\n","        \n","    def forward(self, x):\n","        x = self.drop(F.relu(self.fc1(x)))\n","        x = self.fc2(x)\n","        x = F.log_softmax(x, dim=1)\n","        return x\n","\n","    \n","def load_checkpoint(checkpoint_path):\n","    checkpoint = torch.load(checkpoint_path)\n","    \n","    model = models.vgg16(pretrained=False)\n","    for param in model.parameters():\n","        param.requires_grad = False\n","\n","    # Put the classifier on the pretrained network\n","    model.classifier = FFClassifier(25088, checkpoint['hidden'], 102)\n","    \n","    model.load_state_dict(checkpoint['state_dict'])\n","    \n","    return model\n","\n","model = load_checkpoint('/home/workspace/classifier.pt')\n","```\n","\n","Your exact code here will depend on how you defined your network in the project. Make sure you use the absolute path to your checkpoint which should have been uploaded to the `/home/workspace` directory.\n","\n","Run the cell, then after loading the data, press \"Test Code\" below. This can take a few minutes or more depending on the size of your network. Your model needs  to reach **at least 20% accuracy** on the test set to be recorded."]},{"metadata":{"id":"_upzUWFRUZwT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":972},"outputId":"2f6333ab-9508-44c7-c11b-5b94dd7d417e","executionInfo":{"status":"ok","timestamp":1546463621613,"user_tz":-60,"elapsed":19044,"user":{"displayName":"Manu Garcia","photoUrl":"","userId":"08301947753746264803"}}},"cell_type":"code","source":["# env_is_google_colab\n","# \n","# True to run the script in Google Colab\n","# False to run the script in a local environment\n","# \n","env_is_google_colab = True\n","\n","if env_is_google_colab:\n","  \n","  print('Running in a Google Colaboratory environment.')\n","\n","  from os import path\n","  from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n","  platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n","  accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n","  !pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n","    \n","  from google.colab import drive\n","  drive.mount('/content/gdrive')\n","  \n","  print('Displaying folders ...')\n","  \n","  !ls 'gdrive/My Drive/colab/pytorch_challenge/'\n","    \n","else:\n","  \n","  print('Running in a local environment.')\n","  \n","  \n","# Imports here\n","\n","# Main PyTorch library\n","import torch\n","# Pytorch Neural Network module\n","import torch.nn as nn\n","# Convolution functions\n","import torch.nn.functional as F\n","# Optimizatization algorithms\n","import torch.optim as optim\n","\n","# Torchvision for image transformation and dataset loading\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","import torchvision.models as models\n","import torchvision\n","\n","# General definitions\n","if env_is_google_colab:\n","  base_dir = 'gdrive/My Drive/colab/pytorch_challenge'\n","else:\n","  base_dir = '/home/workspace'\n","  \n","model_checkpoint_path = base_dir + '/model_flowers.pth'\n","    \n","    \n","def create_model():\n","    \n","    model = models.vgg19(pretrained=True)\n","    for param in model.parameters():\n","        param.requires_grad = False\n","    \n","    # Get the current number of input features\n","    num_features = model.classifier[6].in_features\n","\n","    # Freeze the pre-loaded network feature parameters so we don't update it's weights\n","    for param in model.features.parameters():\n","        param.requires_grad = False\n","\n","    # Remove last pre trained layer to be replaced with our classifier\n","    classifier = list(model.classifier.children())[:-1]\n","    # Adds out own classifier, with the same input features as before and \n","    # our number of classes\n","    classifier.extend([nn.Linear(num_features, 102)])\n","    # Replace the model classifier\n","    model.classifier = nn.Sequential(*classifier)\n","    \n","    return model\n","  \n","  \n","# TODO: Write a function that loads a checkpoint and rebuilds the model\n","def load_model(file_path):\n","\n","    # Force all tensors to be on CPU\n","    checkpoint = torch.load(file_path, map_location=lambda storage, loc: storage)\n","    \n","    model = create_model()\n","    \n","    model.load_state_dict(checkpoint['state_dict'])\n","    model.class_to_idx = checkpoint['class_to_idx']\n","    model.idx_to_class = {val: key for key, val in model.class_to_idx.items()}\n","    \n","    return model\n","  \n","  \n","  \n","  \n","# Load your model to this variable\n","model = load_model(model_checkpoint_path)\n","   \n","# If you used something other than 224x224 cropped images, set the correct size here\n","image_size = 224\n","# Values you used for normalizing the images. Default here are for \n","# pretrained models from torchvision.\n","norm_mean = [0.485, 0.456, 0.406]\n","norm_std = [0.229, 0.224, 0.225]\n","\n","model"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Running in a Google Colaboratory environment.\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Displaying folders ...\n"," assets\t\t    flower_data\t\t\t      model_flowers.pth\n"," cat_to_name.json  'Image Classifier Project.ipynb'  'Test Score.ipynb'\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["VGG(\n","  (features): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace)\n","    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace)\n","    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (6): ReLU(inplace)\n","    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (8): ReLU(inplace)\n","    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (11): ReLU(inplace)\n","    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (13): ReLU(inplace)\n","    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (15): ReLU(inplace)\n","    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (17): ReLU(inplace)\n","    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (20): ReLU(inplace)\n","    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (22): ReLU(inplace)\n","    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (24): ReLU(inplace)\n","    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (26): ReLU(inplace)\n","    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (29): ReLU(inplace)\n","    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (31): ReLU(inplace)\n","    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (33): ReLU(inplace)\n","    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (35): ReLU(inplace)\n","    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (classifier): Sequential(\n","    (0): Linear(in_features=25088, out_features=4096, bias=True)\n","    (1): ReLU(inplace)\n","    (2): Dropout(p=0.5)\n","    (3): Linear(in_features=4096, out_features=4096, bias=True)\n","    (4): ReLU(inplace)\n","    (5): Dropout(p=0.5)\n","    (6): Linear(in_features=4096, out_features=102, bias=True)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":16}]}]}